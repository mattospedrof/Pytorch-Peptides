{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Protótipo de Deep Learning para o descobrimento de novos peptídeos\n",
        "- Olá, o objetivo desse trabalho foi desenvolver uma rede neural usando python, pytorch e scikit-learn a fim de avaliar a possibilidade de descoberta de novos peptídeos antimicrobianos.\n",
        "- O código foi estruturado em etapas, desde o tratamento dos dados até a avaliação de novas sequências. As variáveis foram mantidas em inglês para se aproximar com a linguagem das documentações."
      ],
      "metadata": {
        "id": "f4QTgdQflf3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tratamento inicial dos dados\n",
        "\n",
        "- Sequências APD, DBAASP e DRAMP como sequências positivas (AMP).\n",
        "- Sequências do Uniprot como não-AMP.\n",
        "  - Ao procurar por protein no Uniprot, obtemos uma lista muito extensa de sequências. Sendo assim, foi aplicado um filtro 'protein NOT antimicrobial NOT antibiotic NOT antibacterial NOT antiviral NOT antifungal NOT antimalarial NOT antiparasitic NOT anti-protist NOT anticancer NOT defensin NOT defense NOT cathelicidin NOT histatin NOT bacteriocin NOT microbicidal NOT fungicide NOT signal NOT recombination NOT ribosomal NOT secretion NOT DNA NOT RNA AND (length:[* TO 190])'. Com esse filtro, foram obtidas 11.165 sequências (referência https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-022-08310-4#Sec9)."
      ],
      "metadata": {
        "id": "ASZD2Q_HVTsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import OrderedDict\n",
        "from google.colab import files\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "BnVRTtiW-YBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")     # Verificação para saber se há uma GPU disponível"
      ],
      "metadata": {
        "id": "1_z5HFnkacB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6WOErKc5kWS"
      },
      "outputs": [],
      "source": [
        "def calc_hidrofob(sequence):\n",
        "  indx_hidro = {\n",
        "      'A': 0.310,\n",
        "      'R': -1.010,\n",
        "      'N': -0.600,\n",
        "      'D': -0.770,\n",
        "      'C': 1.540,\n",
        "      'Q': -0.220,\n",
        "      'E': -0.640,\n",
        "      'G': 0.000,\n",
        "      'H': 0.130,\n",
        "      'I': 1.800,\n",
        "      'L': 1.700,\n",
        "      'K': -0.990,\n",
        "      'M': 1.230,\n",
        "      'F': 1.790,\n",
        "      'P': 0.720,\n",
        "      'S': -0.040,\n",
        "      'T': 0.260,\n",
        "      'W': 2.250,\n",
        "      'Y': 0.960,\n",
        "      'V': 1.220\n",
        "      }\n",
        "\n",
        "  if isinstance(sequence, str):\n",
        "    sum_hydrophobicity = sum(indx_hidro.get(aa, 0) for aa in sequence)\n",
        "    return sum_hydrophobicity / len(sequence)\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "def calc_net_charge(sequence):\n",
        "  if isinstance(sequence, str):\n",
        "    count_positives = sum(1 for aa in sequence if aa in ['K', 'R'])\n",
        "    count_negatives = sum(1 for aa in sequence if aa in ['D', 'E'])\n",
        "    net_charge = count_positives - count_negatives\n",
        "    return net_charge\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "unificated_sequences = []\n",
        "non_amp_sequences = []\n",
        "sequencias_filtradas = []\n",
        "fully_sequences_dataset = []\n",
        "\n",
        "dbaasp = \"peptides_dbaasp_2-190_all_modificated_targets.csv\"\n",
        "apd = \"APD_sequence_release_09142020.txt\"\n",
        "uniprot_swiss = \"uniprotkb_protein_NOT_antimicrobial_NOT_2024_05_14.xlsx\"\n",
        "comprimento_minimo = 2\n",
        "comprimento_maximo = 190\n",
        "dados_excel_uniprot = pd.read_excel(uniprot_swiss, engine='openpyxl')\n",
        "\n",
        "for index, linha in dados_excel_uniprot.iterrows():\n",
        "    if comprimento_minimo <= linha['Length'] <= comprimento_maximo:\n",
        "        sequencias_filtradas.append(linha['Sequence'])\n",
        "\n",
        "# print(f'Sequências do UNIPROT filtradas: {len(sequencias_filtradas)}')\n",
        "peptides_dbaasp = pd.read_csv(dbaasp)\n",
        "sequences_dbaasp = peptides_dbaasp[\"SEQUENCE\"]\n",
        "\n",
        "for sequence_dbaasp in sequences_dbaasp:\n",
        "    unificated_sequences.append(sequence_dbaasp)\n",
        "\n",
        "try:\n",
        "    with open(f\"{apd}\", \"r\") as arquivo:\n",
        "        data = arquivo.read()\n",
        "        apd_regex = r'(?<=\\))\\s*([A-Z\\s]+)(?=\\s*[A-Z]*$)'\n",
        "        sequences_apd = re.findall(apd_regex, data, re.M)\n",
        "\n",
        "        for sequence in sequences_apd:\n",
        "            # print(f'{sequence} \\t {len(sequence)}')\n",
        "            if unificated_sequences[0] is None:\n",
        "                print(\"Epa, veio nada\")\n",
        "            else:\n",
        "                unificated_sequences.append(sequence)\n",
        "except FileNotFoundError:\n",
        "    print(f\"O arquivo {apd} não foi encontrado.\")\n",
        "except Exception as e:\n",
        "    print(f\"Ocorreu um erro: {e}\")\n",
        "# todas as sequências agora estão em unificated_sequences\n",
        "\n",
        "unificated_sequences_no_duplicate = list(OrderedDict.fromkeys(unificated_sequences))\n",
        "print(f'Número de sequências não duplicadas: {len(unificated_sequences_no_duplicate)}')\n",
        "\n",
        "seq_filt_sem_correspondencias = [seq for seq in sequencias_filtradas if seq not in unificated_sequences_no_duplicate]\n",
        "print(f'Sequências sem correspondência com AMP: {len(seq_filt_sem_correspondencias)}')\n",
        "\n",
        "for sequence_non_amp in seq_filt_sem_correspondencias:\n",
        "    non_amp_sequences.append(sequence_non_amp)\n",
        "\n",
        "print(f'Número de sequências não-AMP: {len(non_amp_sequences)}')\n",
        "\n",
        "amp_hydrophobicity = [calc_hidrofob(seq) for seq in unificated_sequences_no_duplicate]\n",
        "amp_charge = [calc_net_charge(seq) for seq in unificated_sequences_no_duplicate]\n",
        "non_amp_hydrophobicity = [calc_hidrofob(seq) for seq in non_amp_sequences]\n",
        "non_amp_charge = [calc_net_charge(seq) for seq in non_amp_sequences]\n",
        "\n",
        "# Criando um dataframe contendo as sequências, hidrofobicidade, carga e classificação\n",
        "amp_data = pd.DataFrame({\n",
        "    'Sequence': unificated_sequences_no_duplicate,\n",
        "    'Hydrophobicity': amp_hydrophobicity,\n",
        "    'Charge': amp_charge,\n",
        "    'Classification': '1.0'\n",
        "})\n",
        "\n",
        "non_amp_data = pd.DataFrame({\n",
        "    'Sequence': non_amp_sequences,\n",
        "    'Hydrophobicity': non_amp_hydrophobicity,\n",
        "    'Charge': non_amp_charge,\n",
        "    'Classification': '0.0'\n",
        "})\n",
        "\n",
        "# Número de sequências não duplicadas: 15220\n",
        "# Sequências sem correspondência com AMP: 11127\n",
        "# Número de sequências não-AMP: 11127  - 3709 sequências se quisermos dividir em 3 dataframes\n",
        "\n",
        "fully_sequences_dataset = pd.concat([amp_data, non_amp_data], ignore_index=True)\n",
        "shuffled_fully_sequences_dataset = fully_sequences_dataset.sample(frac=1).reset_index(drop=True)\n",
        "shuffled_fully_sequences_dataset\n",
        "\n",
        "\n",
        "# Verificação de maior comprimento de uma sequência AMP - encontrado 190 na execução atual\n",
        "\n",
        "# maior_comprimento = 0\n",
        "# with open(\"Verify.txt\", \"w\") as ver:\n",
        "#     ver.write(f\"Sequences \\t Lenght\\n\")\n",
        "\n",
        "#     for c in unificated_sequences_no_duplicate:\n",
        "#         if isinstance(c, str):\n",
        "#             ver.write(f\"{c} \\t {len(c)}\\n\")\n",
        "#             comprimento_atual = len(c)\n",
        "#             if comprimento_atual > maior_comprimento:\n",
        "#                 maior_comprimento = comprimento_atual\n",
        "\n",
        "# print(f'Maior_comprimento_AMP: {maior_comprimento}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for index, item in enumerate(shuffled_fully_sequences_dataset['Sequence']):\n",
        "    tipo = type(item)\n",
        "    if tipo is not str:\n",
        "        print(f\"Índice: {index}, Conteúdo: {item}, Tipo: {tipo}\")"
      ],
      "metadata": {
        "id": "oS3kQyvvGZwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_fully_sequences_dataset.dropna(inplace=True)\n",
        "shuffled_fully_sequences_dataset"
      ],
      "metadata": {
        "id": "OSr0nCD_Imyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def uppercase_sequence(sequence):\n",
        "    return sequence.upper()\n",
        "\n",
        "# Aplicar a função em todas as sequências presentes no df\n",
        "shuffled_fully_sequences_dataset['Sequence'] = shuffled_fully_sequences_dataset['Sequence'].apply(uppercase_sequence)"
      ],
      "metadata": {
        "id": "69SY-_vfpLPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para transformar as sequências em codificação one_hot\n",
        "def clean_sequence(sequence):\n",
        "    valid_characters = set('ACDEFGHIKLMNPQRSTVWY')\n",
        "    cleaned_sequence = ''.join([char for char in sequence if char in valid_characters])\n",
        "    return cleaned_sequence\n",
        "\n",
        "\n",
        "def one_hot_encoding(peptide_sequence):\n",
        "    amino_acid_index = {'A': 0, 'C': 1, 'D': 2, 'E': 3, 'F': 4,\n",
        "                        'G': 5, 'H': 6, 'I': 7, 'K': 8, 'L': 9,\n",
        "                        'M': 10, 'N': 11, 'P': 12, 'Q': 13, 'R': 14,\n",
        "                        'S': 15, 'T': 16, 'V': 17, 'W': 18, 'Y': 19}\n",
        "\n",
        "    cleaned_sequence = clean_sequence(peptide_sequence)\n",
        "    comp_peptide = len(cleaned_sequence)\n",
        "    one_hot_encoded = np.zeros((comp_peptide, 20))\n",
        "\n",
        "    # Preenchendo a matriz com 1 na posição referente ao aminoácido\n",
        "    for i, amino_acid in enumerate(cleaned_sequence):\n",
        "        if amino_acid in amino_acid_index:\n",
        "            index = amino_acid_index[amino_acid]\n",
        "            one_hot_encoded[i, index] = 1\n",
        "\n",
        "    return one_hot_encoded"
      ],
      "metadata": {
        "id": "Wb82-1SK-ggr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_fully_sequences_dataset['Sequence'] = shuffled_fully_sequences_dataset['Sequence'].astype(str)\n",
        "shuffled_fully_sequences_dataset['Sequence'] = shuffled_fully_sequences_dataset['Sequence'].apply(one_hot_encoding)\n",
        "shuffled_fully_sequences_dataset['Sequence']"
      ],
      "metadata": {
        "id": "wuS9CJiUntdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_tensor(fraction):\n",
        "    # Se o valor for único, converte para um tensor\n",
        "    if isinstance(fraction, float) or isinstance(fraction, int) or isinstance(fraction, str):\n",
        "        return torch.tensor(fraction, dtype=torch.float32)\n",
        "    # Se for uma coleção de valores, converte cada item para um tensor e retorna a lista de tensores\n",
        "    elif isinstance(fraction, list):\n",
        "        return [torch.tensor(item, dtype=torch.float32) for item in fraction]\n",
        "    # Se for uma matriz, converte para tensor\n",
        "    elif isinstance(fraction, np.ndarray):\n",
        "        return torch.tensor(fraction.tolist(), dtype=torch.float32)\n",
        "    else:\n",
        "        raise TypeError(\"Tipo de valor não suportado: \", type(fraction))"
      ],
      "metadata": {
        "id": "AcEZoSHMbUzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_fully_sequences_dataset['Sequence'] = shuffled_fully_sequences_dataset['Sequence'].apply(transform_tensor)\n",
        "shuffled_fully_sequences_dataset['Sequence']"
      ],
      "metadata": {
        "id": "Z7tipV2CrEBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_fully_sequences_dataset['Sequence'][0][0]\n"
      ],
      "metadata": {
        "id": "79uHpmOrsRt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_fully_sequences_dataset['Hydrophobicity'] = shuffled_fully_sequences_dataset['Hydrophobicity'].apply(transform_tensor)\n",
        "shuffled_fully_sequences_dataset['Hydrophobicity']"
      ],
      "metadata": {
        "id": "90fIst3xs7-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_fully_sequences_dataset['Charge'] = shuffled_fully_sequences_dataset['Charge'].apply(transform_tensor)\n",
        "shuffled_fully_sequences_dataset['Charge']"
      ],
      "metadata": {
        "id": "rE8YIA6erk2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_fully_sequences_dataset['Classification'] = shuffled_fully_sequences_dataset['Classification'].astype(float)\n",
        "shuffled_fully_sequences_dataset['Classification'] = shuffled_fully_sequences_dataset['Classification'].apply(transform_tensor)\n",
        "shuffled_fully_sequences_dataset['Classification']"
      ],
      "metadata": {
        "id": "NjxxiXzGsDEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(shuffled_fully_sequences_dataset)"
      ],
      "metadata": {
        "id": "DfAdCjgvtjKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Criação do modelo de rede neural com Pytorch\n",
        "\n",
        "Link de referência para a criação de uma rede LSTM (https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n",
        "\n",
        "- Para a criação, utilizaremos o Pytorch. Poderia utilizar o TensorFlow e Keras? Poderia, mas nesse caso vamos usar uma tecnologia mais recente e que tem se mostrado bem eficiente (o GPT tem uma base no Pytorch).\n",
        "- Como vimos, temos 15628 sequências em matrizes. Alguns autores optariam por fazer uma rede neural com convolução, que nada mais é do que a união das matrizes dos dados, porém, ao longo do processo perdemos parte desses dados, que, no nosso caso, não é interessante, uma vez que se reduzirmos a codificação one_hot, perderemos os dados dos aminoácidos iniciais."
      ],
      "metadata": {
        "id": "o7snJEGytTW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PepModelLSTM(nn.Module):\n",
        "    def __init__(self, input_size=20, hidden_size=10, num_layers=1, num_classes=2):\n",
        "        super(PepModelLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(2 * hidden_size, num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(2 * self.num_layers, x.size(0), self.hidden_size).to(x.device)   # Enviando os dados para o device, seja cuda ou CPU\n",
        "        c0 = torch.zeros(2 * self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        out, _ = self.lstm(x, (h0, c0))  # out: Tupla do tensor representada por (batch_size, seq_length, 2 * hidden_size) devido à bidirecionalidade\n",
        "\n",
        "        # Considerando apenas a saída do último passo de tempo e descartando a outra direção\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "lHpVBwlwVS6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exibir as primeiras amostras de cada coluna para confirmar se os dados estão certos\n",
        "\n",
        "# for col in shuffled_fully_sequences_dataset.columns:\n",
        "#     print(f\"Primeiras amostras da coluna '{col}':\")\n",
        "#     for sample in shuffled_fully_sequences_dataset[col][:5]:\n",
        "#         print(sample)"
      ],
      "metadata": {
        "id": "ADGNSZ-yNpY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = shuffled_fully_sequences_dataset['Sequence']\n",
        "y = shuffled_fully_sequences_dataset['Classification']\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "4GhXilSFzLHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sequences(sequences, max_length):\n",
        "    padded_sequences = []\n",
        "    empty_indices = []\n",
        "    for i, seq in enumerate(sequences):\n",
        "        seq_len = len(seq)\n",
        "        if seq_len == 0:\n",
        "            empty_indices.append(i)\n",
        "            padded_sequences.append(seq)\n",
        "        elif seq_len <= max_length:\n",
        "            padding = torch.zeros(max_length - seq_len, seq.shape[1])\n",
        "            padded_seq = torch.cat([seq, padding])\n",
        "            padded_sequences.append(padded_seq)\n",
        "        else:\n",
        "            padded_sequences.append(seq[:max_length])\n",
        "    return padded_sequences, empty_indices"
      ],
      "metadata": {
        "id": "qo0QLRbB1MbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_length = 190  # Comprimento máximo encontrado\n",
        "X_padded, empty_indices = pad_sequences(X.values, max_sequence_length)\n",
        "print(\"Índices das sequências vazias:\", empty_indices)"
      ],
      "metadata": {
        "id": "YN8e1Gy4vYgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = y.astype(np.float32)\n",
        "X_tensor = torch.stack(X_padded).to(device)\n",
        "y_tensor = torch.tensor(y.values, dtype=torch.float32).to(device)"
      ],
      "metadata": {
        "id": "g-e_gAsO_0VV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model = 'pep_lstm_re_trained.pth'\n",
        "model = PepModelLSTM().to(device)\n",
        "model.load_state_dict(torch.load(saved_model))\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "epochs = 20000\n",
        "losses = []\n",
        "\n",
        "num_classes = 2\n",
        "y_tensor_one_hot = F.one_hot(y_tensor.to(torch.int64), num_classes)"
      ],
      "metadata": {
        "id": "v7Fyq4Wf5Gwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(epochs):\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  y_pred = model.forward(X_tensor)\n",
        "  loss = criterion(y_pred, y_tensor_one_hot.float())\n",
        "\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  losses.append(loss)\n",
        "  if i % 100 == 0:\n",
        "    print(f'Epoch: {i} and loss: {loss:.6f}')"
      ],
      "metadata": {
        "id": "wwJgCSitMTdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "model.eval()    # Modo de avaliação\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(X_tensor):\n",
        "        y_val = model(data.unsqueeze(0))\n",
        "        predicted_class = y_val.argmax().item()\n",
        "        true_class = y_tensor[i].item()\n",
        "\n",
        "        print(f'{i+1}.) Predicted: {predicted_class}, True: {true_class}')\n",
        "\n",
        "        if predicted_class == true_class:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "\n",
        "print(f'Nós temos {correct} previsões corretas em um total de {total} sequências.')"
      ],
      "metadata": {
        "id": "avhXKhLoWveR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()    # Modo de avaliação\n",
        "\n",
        "# Lista para armazenar os dados previstos e reais\n",
        "predicted_labels = []\n",
        "true_labels = []\n",
        "predicted_probabilities = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(X_tensor):\n",
        "        y_val = model(data.unsqueeze(0))\n",
        "\n",
        "        # Saída da rede neural\n",
        "        predicted_class = y_val.argmax().item()\n",
        "        # Probabilidade de ser 1.0\n",
        "        predicted_probability = torch.sigmoid(y_val[:, 1]).item()\n",
        "        predicted_probabilities.append(predicted_probability)\n",
        "        # Saída real\n",
        "        true_class = y_tensor[i].item()\n",
        "        # Armazenando as previsões e os verdadeiros\n",
        "        predicted_labels.append(predicted_class)\n",
        "        true_labels.append(true_class)\n",
        "\n",
        "# Calculando as métricas do sk-learn\n",
        "precision = precision_score(true_labels, predicted_labels)\n",
        "recall = recall_score(true_labels, predicted_labels)\n",
        "f1 = f1_score(true_labels, predicted_labels)\n",
        "auroc = roc_auc_score(true_labels, predicted_probabilities)\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "gini_index = (2 * auroc) - 1\n",
        "\n",
        "# Exibindo os resultados\n",
        "print(f'Precisão: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1-Score: {f1:.4f}')\n",
        "print(f'AUROC: {auroc:.4f}')\n",
        "print(f'Acurácia: {accuracy:.4f}')\n",
        "print(f'Índice de Gini: {gini_index:.4f}')"
      ],
      "metadata": {
        "id": "CAAkUByu5Uos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = [loss.cpu().item() for loss in losses]\n",
        "\n",
        "plt.plot(range(epochs), losses)\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pP5i2SqlfbPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "std_deviation = np.std(losses)\n",
        "print(f\"Desvio padrão das perdas: {std_deviation:.3f}\")"
      ],
      "metadata": {
        "id": "iMWFWvtZM4a8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Como salvar o modelo\n",
        "torch.save(model.state_dict(), 'pep_lstm.pth')\n",
        "files.download('pep_lstm.pth')"
      ],
      "metadata": {
        "id": "ysYyvdAqyOxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Como carregar o modelo\n",
        "model = PepModelLSTM().to(device)\n",
        "model.load_state_dict(torch.load('pep_lstm_re_trained.pth'))\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "Gv9EgG_9h0z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sequence_to_one_hot(sequence, amino_acids):\n",
        "    one_hot = torch.zeros(len(sequence), len(amino_acids))\n",
        "    for i, aa in enumerate(sequence):\n",
        "        one_hot[i, amino_acids.index(aa)] = 1\n",
        "    return one_hot\n",
        "\n",
        "\n",
        "def evaluate_sequences(model, sequences, amino_acids):\n",
        "    probabilities = []\n",
        "    with torch.no_grad():\n",
        "        for sequence in sequences:\n",
        "            input_sequence = sequence_to_one_hot(sequence, amino_acids)\n",
        "            input_tensor = torch.tensor(input_sequence, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            output = model(input_tensor)\n",
        "            probability = torch.sigmoid(output[:, 1]).item()\n",
        "            probabilities.append(probability)\n",
        "    return probabilities\n",
        "\n",
        "\n",
        "def generate_random_sequences(num_sequences, sequence_length, amino_acids):\n",
        "    sequences = []\n",
        "    for _ in range(num_sequences):\n",
        "        sequence = ''.join(random.choices(amino_acids, k=sequence_length))\n",
        "        sequences.append(sequence)\n",
        "    return sequences\n",
        "\n",
        "num_sequences = 10  # Número de sequências que você quer gerar, altere conforme seu critério.\n",
        "sequence_length = input(\"Qual o número de aminoácidos que sua sequência deve ter?\"\n",
        "                        \"(Max=190)=> \")     # Nesse input, utilizamos o maior comprimento encontrado\n",
        "sequence_length = int(sequence_length)\n",
        "threshold = 0.99    # Probabilidade mínima utilizada, altere conforme seu critério.\n",
        "high_prob_sequences = []\n",
        "\n",
        "while len(high_prob_sequences) < num_sequences:\n",
        "    amino_acids = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
        "    sequences = generate_random_sequences(num_sequences, sequence_length, amino_acids)\n",
        "    probabilities = evaluate_sequences(model, sequences, amino_acids)\n",
        "\n",
        "    for sequence, prob in zip(sequences, probabilities):\n",
        "        if prob >= threshold:\n",
        "            high_prob_sequences.append((sequence, prob))\n",
        "            if len(high_prob_sequences) == num_sequences:\n",
        "                break\n",
        "\n",
        "high_prob_sequences_sorted = sorted(high_prob_sequences, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"Sequências com maior probabilidade AMP:\")\n",
        "for i, (sequence, prob) in enumerate(high_prob_sequences_sorted, start=1):\n",
        "    print(f\"{i}: Sequence: {sequence} \\t Charge: {calc_net_charge(sequence)} \\t <H>: {calc_hidrofob(sequence):.3f} \\t Probability: {prob*100:.3f} %\")"
      ],
      "metadata": {
        "id": "cgqiu8v-h1VI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nessa célula o usuário pode inserir uma sequência e ver sua probabilidade\n",
        "user_sequence = input(\"Insira a sequência de aminoácidos: \")\n",
        "user_probability = evaluate_sequences(model, [user_sequence], amino_acids)[0]\n",
        "print(f\"Probabilidade da sequência inserida ser AMP: {user_probability*100:.3f} %\")"
      ],
      "metadata": {
        "id": "oIOBfiZn1qrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()      # 'Limpar' a memória da GPU"
      ],
      "metadata": {
        "id": "a1QmXZdx0ZYF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}